from collections import defaultdict

# Sample text corpus
corpus = [
    "I am Sam",
    "Sam I am",
    "I do not like green eggs and ham",
    "I like green eggs",
    "Do you like green eggs and ham?"
]

# Initialize counters
bigram_counts = defaultdict(int)
unigram_counts = defaultdict(int)

# Process each sentence in the corpus
for sentence in corpus:
    words = sentence.lower().split()  # Tokenize and convert to lowercase
    for i in range(len(words) - 1):
        bigram_counts[(words[i], words[i + 1])] += 1
        unigram_counts[words[i]] += 1
    unigram_counts[words[-1]] += 1  # Count the last word

# Calculate bigram probabilities
bigram_probabilities = {}
for (w1, w2), count in bigram_counts.items():
    bigram_probabilities[(w1, w2)] = count / unigram_counts[w1]

# Function to predict the next word
def predict_next_word(last_word):
    candidates = {w2: bigram_probabilities.get((last_word, w2), 0) for w2 in unigram_counts.keys()}
    # Sort candidates by probability
    sorted_candidates = sorted(candidates.items(), key=lambda item: item[1], reverse=True)
    return sorted_candidates

# Example usage
last_word = "am"
predictions = predict_next_word(last_word)
print(f"Next word predictions for '{last_word}':")
for word, prob in predictions:
       if prob > 0:
  # Only show words with a non-zero probability
        print(f"  {word}: {prob:.2f}")

Output:
Next word predictions for 'am':
  sam: 0.50
